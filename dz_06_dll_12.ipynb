{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4806abde",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ce63a1c4",
   "metadata": {},
   "source": [
    "#### Домашнее задание\n",
    "* Сгенерировать последовательности, которые бы состояли из цифр (от 0 до 9) которые бы\n",
    "  задавались следующим образом:\n",
    "* x - последовательность цифр\n",
    "* y1 = x1, y(i) = x(i) + x(1). Если y(i) >= 10, то y(i) = y(i) - 10\n",
    "* Задача научить модель предсказывать y(i) по x(i)\n",
    "* Опробовать RNN, LSTM, GRU длинной 10, 25, 50 , 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a920300",
   "metadata": {
    "id": "iHah9Vq74t0e"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import re\n",
    "import random\n",
    "import tqdm\n",
    "import time\n",
    "import numpy as np\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2b19aff",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e70ae30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def num(num):\n",
    "    x = torch.randint(0, 9, (num, ))\n",
    "    y = torch.zeros(x.shape[0]).int()\n",
    "    for i, j in enumerate(x):\n",
    "        if i == 0:\n",
    "            y[i] = x[i]\n",
    "        else:\n",
    "            y[i] = x[i] + x[0]\n",
    "            if y[i] >= 10:\n",
    "                y[i] -= 10\n",
    "    x = x.reshape(1, num)\n",
    "    y = y.reshape(1, num)            \n",
    "    return x, y  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "643436ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1, 3, 4, 6, 6]]), tensor([[1, 4, 5, 7, 7]], dtype=torch.int32))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "142b3b48",
   "metadata": {
    "id": "psIcSGM27YPL"
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self,  rnnClass, dictionary_size, embedding_size = 10, num_hiddens = 128, num_classes = 10):\n",
    "        super().__init__()\n",
    "        self.rnnClass = rnnClass\n",
    "        self.num_hiddens = num_hiddens\n",
    "        self.embedding = nn.Embedding(dictionary_size, embedding_size)\n",
    "        self.hidden = rnnClass(embedding_size, num_hiddens)\n",
    "        self.output = nn.Linear(num_hiddens, num_classes)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        out = self.embedding(X)\n",
    "        _, state = self.hidden(out)\n",
    "        if self.rnnClass == nn.LSTM:\n",
    "            predictions = self.output(state[0][0])\n",
    "        else:\n",
    "            predictions = self.output(state[0])\n",
    "\n",
    "        return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ca72ef9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(preds):\n",
    "    soft = nn.LogSoftmax(dim=1)    \n",
    "    softmaxed = soft(preds)\n",
    "    probas = torch.distributions.multinomial.Multinomial(1, softmaxed).sample()\n",
    "    return probas.argmax(1)\n",
    "\n",
    "def generate_num(i):\n",
    "    x, y = num(i)\n",
    "    for j in range(i):\n",
    "\n",
    "        preds = model((x[0][j]).reshape(1, 1))\n",
    "        \n",
    "    return sample(preds), y[0][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6e94b545",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = nn.RNN\n",
    "gru = nn.GRU\n",
    "lstm = nn.LSTM\n",
    "nets = {'RNN': rnn, 'GRU': gru, 'LSTM': lstm}\n",
    "d = [10, 25 , 50, 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "55b7964c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qshorynU9-Cx",
    "outputId": "e2b45154-9cf2-4799-d66a-2eb9fc5a92fb"
   },
   "outputs": [],
   "source": [
    "# for k, v in net.items():\n",
    "#     model = NeuralNetwork(v, 10, 10, 128, 10)\n",
    "\n",
    "def net_model(k, v, i, model):\n",
    "    \n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "    print(\"_\" * 20)\n",
    "    print(\"модель:\", k, \"Длина:\", i)\n",
    "    \n",
    "    rnn_dict = {}\n",
    "    x, y = num(i)\n",
    "    emb = nn.Embedding(10, 10)\n",
    "    for ep in range(10):\n",
    "        for h in range(i):\n",
    "                   \n",
    "\n",
    "            n = x[0][h].reshape(1, 1)\n",
    "            j = y[0][h].view(1,)\n",
    "\n",
    "            start = time.time()\n",
    "            train_loss = 0.\n",
    "            train_passed = 0\n",
    "\n",
    "            model.train() \n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            answers = model(n)  \n",
    "\n",
    "\n",
    "            loss = criterion(answers, emb(j))\n",
    "            train_loss += loss.item()\n",
    "\n",
    "\n",
    "            loss.backward(retain_graph = True)    \n",
    "            optimizer.step()\n",
    "            train_passed += 1    \n",
    "\n",
    "        print(\"Epoch {}. Time: {:.3f}, Train loss: {:.3f}\".format(ep, time.time() - start, train_loss / train_passed))\n",
    "        model.eval()\n",
    "        gen = generate_num(i)\n",
    "        l = np.where((gen[0].squeeze() - gen[1].squeeze())==0)    \n",
    "        print(f' \"Точность\"', abs((10-abs(gen[0].squeeze() - gen[1].squeeze())).item())*10, '%')\n",
    "        print((gen[0].squeeze(), gen[1].squeeze()))\n",
    "        if (gen[0].squeeze() - gen[1].squeeze()).item() == 0:\n",
    "            return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "82a7cf4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________\n",
      "модель: RNN Длина: 10\n",
      "Epoch 0. Time: 0.001, Train loss: -0.777\n",
      " \"Точность\" 60 %\n",
      "(tensor(7), tensor(3, dtype=torch.int32))\n",
      "Epoch 1. Time: 0.001, Train loss: -1.769\n",
      " \"Точность\" 70 %\n",
      "(tensor(3), tensor(0, dtype=torch.int32))\n",
      "Epoch 2. Time: 0.001, Train loss: -2.882\n",
      " \"Точность\" 100 %\n",
      "(tensor(5), tensor(5, dtype=torch.int32))\n",
      "____________________\n",
      "модель: RNN Длина: 25\n",
      "Epoch 0. Time: 0.002, Train loss: -11.518\n",
      " \"Точность\" 100 %\n",
      "(tensor(0), tensor(0, dtype=torch.int32))\n",
      "____________________\n",
      "модель: RNN Длина: 50\n",
      "Epoch 0. Time: 0.001, Train loss: -0.052\n",
      " \"Точность\" 80 %\n",
      "(tensor(1), tensor(3, dtype=torch.int32))\n",
      "Epoch 1. Time: 0.001, Train loss: -0.995\n",
      " \"Точность\" 60 %\n",
      "(tensor(4), tensor(8, dtype=torch.int32))\n",
      "Epoch 2. Time: 0.001, Train loss: -2.210\n",
      " \"Точность\" 10 %\n",
      "(tensor(0), tensor(9, dtype=torch.int32))\n",
      "Epoch 3. Time: 0.001, Train loss: -4.372\n",
      " \"Точность\" 80 %\n",
      "(tensor(6), tensor(8, dtype=torch.int32))\n",
      "Epoch 4. Time: 0.001, Train loss: -7.774\n",
      " \"Точность\" 100 %\n",
      "(tensor(0), tensor(0, dtype=torch.int32))\n",
      "____________________\n",
      "модель: RNN Длина: 100\n",
      "Epoch 0. Time: 0.001, Train loss: 5.833\n",
      " \"Точность\" 80 %\n",
      "(tensor(2), tensor(0, dtype=torch.int32))\n",
      "Epoch 1. Time: 0.001, Train loss: 6.201\n",
      " \"Точность\" 70 %\n",
      "(tensor(4), tensor(7, dtype=torch.int32))\n",
      "Epoch 2. Time: 0.001, Train loss: 7.655\n",
      " \"Точность\" 40 %\n",
      "(tensor(0), tensor(6, dtype=torch.int32))\n",
      "Epoch 3. Time: 0.001, Train loss: 7.213\n",
      " \"Точность\" 90 %\n",
      "(tensor(8), tensor(9, dtype=torch.int32))\n",
      "Epoch 4. Time: 0.001, Train loss: 2.018\n",
      " \"Точность\" 80 %\n",
      "(tensor(2), tensor(4, dtype=torch.int32))\n",
      "Epoch 5. Time: 0.001, Train loss: -8.663\n",
      " \"Точность\" 80 %\n",
      "(tensor(6), tensor(4, dtype=torch.int32))\n",
      "Epoch 6. Time: 0.001, Train loss: -19.213\n",
      " \"Точность\" 90 %\n",
      "(tensor(8), tensor(7, dtype=torch.int32))\n",
      "Epoch 7. Time: 0.001, Train loss: -32.460\n",
      " \"Точность\" 50 %\n",
      "(tensor(8), tensor(3, dtype=torch.int32))\n",
      "Epoch 8. Time: 0.001, Train loss: -46.144\n",
      " \"Точность\" 60 %\n",
      "(tensor(7), tensor(3, dtype=torch.int32))\n",
      "Epoch 9. Time: 0.001, Train loss: -57.675\n",
      " \"Точность\" 40 %\n",
      "(tensor(2), tensor(8, dtype=torch.int32))\n",
      "____________________\n",
      "модель: GRU Длина: 10\n",
      "Epoch 0. Time: 0.002, Train loss: -1.188\n",
      " \"Точность\" 70 %\n",
      "(tensor(0), tensor(3, dtype=torch.int32))\n",
      "Epoch 1. Time: 0.002, Train loss: -1.379\n",
      " \"Точность\" 70 %\n",
      "(tensor(1), tensor(4, dtype=torch.int32))\n",
      "Epoch 2. Time: 0.001, Train loss: -1.583\n",
      " \"Точность\" 90 %\n",
      "(tensor(7), tensor(8, dtype=torch.int32))\n",
      "Epoch 3. Time: 0.001, Train loss: -1.796\n",
      " \"Точность\" 40 %\n",
      "(tensor(6), tensor(0, dtype=torch.int32))\n",
      "Epoch 4. Time: 0.002, Train loss: -2.021\n",
      " \"Точность\" 60 %\n",
      "(tensor(8), tensor(4, dtype=torch.int32))\n",
      "Epoch 5. Time: 0.002, Train loss: -2.267\n",
      " \"Точность\" 60 %\n",
      "(tensor(3), tensor(7, dtype=torch.int32))\n",
      "Epoch 6. Time: 0.001, Train loss: -2.542\n",
      " \"Точность\" 50 %\n",
      "(tensor(1), tensor(6, dtype=torch.int32))\n",
      "Epoch 7. Time: 0.001, Train loss: -2.855\n",
      " \"Точность\" 80 %\n",
      "(tensor(2), tensor(0, dtype=torch.int32))\n",
      "Epoch 8. Time: 0.001, Train loss: -3.213\n",
      " \"Точность\" 40 %\n",
      "(tensor(2), tensor(8, dtype=torch.int32))\n",
      "Epoch 9. Time: 0.001, Train loss: -3.626\n",
      " \"Точность\" 30 %\n",
      "(tensor(7), tensor(0, dtype=torch.int32))\n",
      "____________________\n",
      "модель: GRU Длина: 25\n",
      "Epoch 0. Time: 0.001, Train loss: 3.631\n",
      " \"Точность\" 70 %\n",
      "(tensor(1), tensor(4, dtype=torch.int32))\n",
      "Epoch 1. Time: 0.001, Train loss: 2.907\n",
      " \"Точность\" 70 %\n",
      "(tensor(3), tensor(0, dtype=torch.int32))\n",
      "Epoch 2. Time: 0.002, Train loss: 1.949\n",
      " \"Точность\" 60 %\n",
      "(tensor(1), tensor(5, dtype=torch.int32))\n",
      "Epoch 3. Time: 0.001, Train loss: 0.663\n",
      " \"Точность\" 40 %\n",
      "(tensor(2), tensor(8, dtype=torch.int32))\n",
      "Epoch 4. Time: 0.001, Train loss: -0.936\n",
      " \"Точность\" 80 %\n",
      "(tensor(3), tensor(5, dtype=torch.int32))\n",
      "Epoch 5. Time: 0.002, Train loss: -2.741\n",
      " \"Точность\" 60 %\n",
      "(tensor(4), tensor(0, dtype=torch.int32))\n",
      "Epoch 6. Time: 0.001, Train loss: -4.634\n",
      " \"Точность\" 100 %\n",
      "(tensor(3), tensor(3, dtype=torch.int32))\n",
      "____________________\n",
      "модель: GRU Длина: 50\n",
      "Epoch 0. Time: 0.001, Train loss: 5.264\n",
      " \"Точность\" 30 %\n",
      "(tensor(9), tensor(2, dtype=torch.int32))\n",
      "Epoch 1. Time: 0.001, Train loss: 4.353\n",
      " \"Точность\" 100 %\n",
      "(tensor(2), tensor(2, dtype=torch.int32))\n",
      "____________________\n",
      "модель: GRU Длина: 100\n",
      "Epoch 0. Time: 0.001, Train loss: -5.058\n",
      " \"Точность\" 10 %\n",
      "(tensor(0), tensor(9, dtype=torch.int32))\n",
      "Epoch 1. Time: 0.002, Train loss: -13.649\n",
      " \"Точность\" 40 %\n",
      "(tensor(2), tensor(8, dtype=torch.int32))\n",
      "Epoch 2. Time: 0.001, Train loss: -26.976\n",
      " \"Точность\" 70 %\n",
      "(tensor(5), tensor(8, dtype=torch.int32))\n",
      "Epoch 3. Time: 0.003, Train loss: -41.824\n",
      " \"Точность\" 70 %\n",
      "(tensor(9), tensor(6, dtype=torch.int32))\n",
      "Epoch 4. Time: 0.002, Train loss: -55.988\n",
      " \"Точность\" 100 %\n",
      "(tensor(3), tensor(3, dtype=torch.int32))\n",
      "____________________\n",
      "модель: LSTM Длина: 10\n",
      "Epoch 0. Time: 0.002, Train loss: -3.807\n",
      " \"Точность\" 30 %\n",
      "(tensor(8), tensor(1, dtype=torch.int32))\n",
      "Epoch 1. Time: 0.002, Train loss: -4.038\n",
      " \"Точность\" 60 %\n",
      "(tensor(6), tensor(2, dtype=torch.int32))\n",
      "Epoch 2. Time: 0.002, Train loss: -4.313\n",
      " \"Точность\" 30 %\n",
      "(tensor(0), tensor(7, dtype=torch.int32))\n",
      "Epoch 3. Time: 0.002, Train loss: -4.650\n",
      " \"Точность\" 20 %\n",
      "(tensor(9), tensor(1, dtype=torch.int32))\n",
      "Epoch 4. Time: 0.002, Train loss: -5.080\n",
      " \"Точность\" 80 %\n",
      "(tensor(7), tensor(9, dtype=torch.int32))\n",
      "Epoch 5. Time: 0.002, Train loss: -5.643\n",
      " \"Точность\" 70 %\n",
      "(tensor(8), tensor(5, dtype=torch.int32))\n",
      "Epoch 6. Time: 0.003, Train loss: -6.390\n",
      " \"Точность\" 80 %\n",
      "(tensor(4), tensor(2, dtype=torch.int32))\n",
      "Epoch 7. Time: 0.002, Train loss: -7.391\n",
      " \"Точность\" 70 %\n",
      "(tensor(4), tensor(7, dtype=torch.int32))\n",
      "Epoch 8. Time: 0.002, Train loss: -8.734\n",
      " \"Точность\" 100 %\n",
      "(tensor(2), tensor(2, dtype=torch.int32))\n",
      "____________________\n",
      "модель: LSTM Длина: 25\n",
      "Epoch 0. Time: 0.002, Train loss: -1.233\n",
      " \"Точность\" 70 %\n",
      "(tensor(2), tensor(5, dtype=torch.int32))\n",
      "Epoch 1. Time: 0.002, Train loss: -1.405\n",
      " \"Точность\" 70 %\n",
      "(tensor(2), tensor(5, dtype=torch.int32))\n",
      "Epoch 2. Time: 0.002, Train loss: -1.587\n",
      " \"Точность\" 100 %\n",
      "(tensor(0), tensor(0, dtype=torch.int32))\n",
      "____________________\n",
      "модель: LSTM Длина: 50\n",
      "Epoch 0. Time: 0.002, Train loss: 5.253\n",
      " \"Точность\" 100 %\n",
      "(tensor(4), tensor(4, dtype=torch.int32))\n",
      "____________________\n",
      "модель: LSTM Длина: 100\n",
      "Epoch 0. Time: 0.002, Train loss: 2.958\n",
      " \"Точность\" 90 %\n",
      "(tensor(8), tensor(9, dtype=torch.int32))\n",
      "Epoch 1. Time: 0.002, Train loss: 2.449\n",
      " \"Точность\" 70 %\n",
      "(tensor(6), tensor(9, dtype=torch.int32))\n",
      "Epoch 2. Time: 0.002, Train loss: 5.651\n",
      " \"Точность\" 90 %\n",
      "(tensor(5), tensor(4, dtype=torch.int32))\n",
      "Epoch 3. Time: 0.003, Train loss: 8.400\n",
      " \"Точность\" 90 %\n",
      "(tensor(3), tensor(4, dtype=torch.int32))\n",
      "Epoch 4. Time: 0.003, Train loss: 6.164\n",
      " \"Точность\" 60 %\n",
      "(tensor(4), tensor(8, dtype=torch.int32))\n",
      "Epoch 5. Time: 0.002, Train loss: -4.211\n",
      " \"Точность\" 10 %\n",
      "(tensor(9), tensor(0, dtype=torch.int32))\n",
      "Epoch 6. Time: 0.001, Train loss: -6.812\n",
      " \"Точность\" 80 %\n",
      "(tensor(1), tensor(3, dtype=torch.int32))\n",
      "Epoch 7. Time: 0.002, Train loss: -8.905\n",
      " \"Точность\" 100 %\n",
      "(tensor(8), tensor(8, dtype=torch.int32))\n"
     ]
    }
   ],
   "source": [
    "for k, v in nets.items():\n",
    "    for i in d:\n",
    "        model = NeuralNetwork(v, i)\n",
    "        net_model(k, v, i, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "337bef61",
   "metadata": {},
   "source": [
    "#### Задание 2:\n",
    "##### применить LSTM для решения лекционного практического задания"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457d02ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "14e5a3b0",
   "metadata": {
    "id": "iHah9Vq74t0e"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import re\n",
    "import random\n",
    "import tqdm\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a148b9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S-bs1_g342Nh",
    "outputId": "6d545c46-3681-42bb-f112-af7e6ee097a6"
   },
   "outputs": [],
   "source": [
    "!wget https://s3.amazonaws.com/text-datasets/nietzsche.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d4ff6149",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ArE9Sysh5EDE",
    "outputId": "56093403-a26c-49cf-997d-ebf443e77cc5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length: 600901\n"
     ]
    }
   ],
   "source": [
    "with open('nietzsche.txt', encoding='utf-8') as f:\n",
    "    text = f.read().lower()\n",
    "print('length:', len(text))\n",
    "text = re.sub('[^a-z ]', ' ', text)\n",
    "text = re.sub('\\s+', ' ', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f206bd10",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "Ijyo7gcL49kz",
    "outputId": "629828d4-a850-467c-aca5-1969161bd5dc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'preface supposing that truth is a woman what then is there not ground for suspecting that all philos'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bd48ebb3",
   "metadata": {
    "id": "iNiH2xET5HxF"
   },
   "outputs": [],
   "source": [
    "INDEX_TO_CHAR = sorted(list(set(text)))\n",
    "CHAR_TO_INDEX = {c: i for i, c in enumerate(INDEX_TO_CHAR)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "638bdcbf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GAer4W2RYfhr",
    "outputId": "d1a4abb5-0195-469b-962a-e4bb17e47053"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{' ': 0,\n",
       " 'a': 1,\n",
       " 'b': 2,\n",
       " 'c': 3,\n",
       " 'd': 4,\n",
       " 'e': 5,\n",
       " 'f': 6,\n",
       " 'g': 7,\n",
       " 'h': 8,\n",
       " 'i': 9,\n",
       " 'j': 10,\n",
       " 'k': 11,\n",
       " 'l': 12,\n",
       " 'm': 13,\n",
       " 'n': 14,\n",
       " 'o': 15,\n",
       " 'p': 16,\n",
       " 'q': 17,\n",
       " 'r': 18,\n",
       " 's': 19,\n",
       " 't': 20,\n",
       " 'u': 21,\n",
       " 'v': 22,\n",
       " 'w': 23,\n",
       " 'x': 24,\n",
       " 'y': 25,\n",
       " 'z': 26}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CHAR_TO_INDEX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e20e32e3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X4EeJBub5ueL",
    "outputId": "610dd196-6fe8-4677-db4b-55610762c9c1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num sents: 193075\n"
     ]
    }
   ],
   "source": [
    "MAX_LEN = 40\n",
    "STEP = 3\n",
    "SENTENCES = []\n",
    "NEXT_CHARS = []\n",
    "for i in range(0, len(text) - MAX_LEN, STEP):\n",
    "    SENTENCES.append(text[i: i + MAX_LEN])\n",
    "    NEXT_CHARS.append(text[i + MAX_LEN])\n",
    "print('Num sents:', len(SENTENCES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "58abac69",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EHPHQII_6MUV",
    "outputId": "690f7de6-bba7-4b14-b474-a08f2a77c779"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorization...\n"
     ]
    }
   ],
   "source": [
    "print('Vectorization...')\n",
    "X = torch.zeros((len(SENTENCES), MAX_LEN), dtype=int)\n",
    "Y = torch.zeros((len(SENTENCES)), dtype=int)\n",
    "for i, sentence in enumerate(SENTENCES):\n",
    "    for t, char in enumerate(sentence):\n",
    "        X[i, t] = CHAR_TO_INDEX[char]\n",
    "    Y[i] = CHAR_TO_INDEX[NEXT_CHARS[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3a529cee",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n7MP7Jzi7PAN",
    "outputId": "9d9e268e-1bfe-4b9b-9620-580cd6a462ae"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[16, 18,  5,  6,  1,  3,  5,  0, 19, 21, 16, 16, 15, 19,  9, 14,  7,  0,\n",
       "          20,  8,  1, 20,  0, 20, 18, 21, 20,  8,  0,  9, 19,  0,  1,  0, 23, 15,\n",
       "          13,  1, 14,  0]]),\n",
       " tensor(23))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0:1], Y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c6f9b595",
   "metadata": {
    "id": "4XKb2CyB6nwL"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE=512\n",
    "dataset = torch.utils.data.TensorDataset(X, Y)\n",
    "data = torch.utils.data.DataLoader(dataset, BATCH_SIZE, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7fca3611",
   "metadata": {
    "id": "psIcSGM27YPL"
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, rnnClass, dictionary_size, embedding_size, num_hiddens, num_classes):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.num_hiddens = num_hiddens\n",
    "        self.embedding = nn.Embedding(dictionary_size, embedding_size)\n",
    "        self.hidden = rnnClass(embedding_size, num_hiddens, batch_first=True)\n",
    "        self.output = nn.Linear(num_hiddens, num_classes)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        out = self.embedding(X)\n",
    "        _, state = self.hidden(out)\n",
    "        predictions = self.output(state[0].squeeze())\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b8477a9d",
   "metadata": {
    "id": "wHDuSE8A7ssc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuralNetwork(\n",
       "  (embedding): Embedding(27, 64)\n",
       "  (hidden): LSTM(64, 128, batch_first=True)\n",
       "  (output): Linear(in_features=128, out_features=27, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = NeuralNetwork(nn.LSTM, len(CHAR_TO_INDEX), 64, 128, len(CHAR_TO_INDEX))\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f0450545",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SvKPD9L9zJal",
    "outputId": "3ce9ec61-a815-4630-c868-716fb08f473b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([193075, 40])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "35de646e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vTCG-ESC74UK",
    "outputId": "8a99023f-d495-4ccb-8688-251213a1dcc7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-6.5505e-02, -2.6092e-02,  3.8988e-02, -1.0529e-01,  2.8476e-02,\n",
       "        -2.0989e-02, -9.6306e-02,  1.1347e-01,  1.1606e-01,  1.0811e-01,\n",
       "         1.5207e-01,  1.2353e-01, -4.6715e-02, -1.0129e-01,  1.2708e-02,\n",
       "        -1.3236e-01, -9.8945e-02,  1.0930e-01,  1.2650e-01, -1.4413e-01,\n",
       "        -1.5080e-02,  3.8489e-02,  4.9186e-02,  7.7933e-02, -3.3575e-02,\n",
       "        -9.5032e-05,  9.1126e-03], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(X[0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8782fe0d",
   "metadata": {
    "id": "gch6FQl8x6Hj"
   },
   "outputs": [],
   "source": [
    "# embedding = nn.Embedding(len(INDEX_TO_CHAR), 15)\n",
    "# rnn = nn.LSTM(15,128, batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abed6405",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qbiqECBCP4Bv",
    "outputId": "3503dc33-6852-4f7a-9db0-d3fc938475cd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([10, 40, 128]), torch.Size([10, 128]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rnn = nn.GRU(15,128, batch_first=True)\n",
    "# o, s = rnn(embedding(X[0:10]))\n",
    "# o.shape, s[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f7f265",
   "metadata": {
    "id": "evDHlyNOykBr"
   },
   "outputs": [],
   "source": [
    "# o, s = rnn(embedding(X[0:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d411620",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5hYTukwkykHJ",
    "outputId": "f75b46be-f6ab-4528-8e1c-b0e47dba0b06"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([10, 40, 128]), torch.Size([1, 10, 128]), torch.Size([1, 10, 128]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# o.shape, s[0].shape, s[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd012c0",
   "metadata": {
    "id": "jbeKFkwdFclg"
   },
   "outputs": [],
   "source": [
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c352a167",
   "metadata": {
    "id": "ZQpkKJV_76dq"
   },
   "outputs": [],
   "source": [
    "def sample(preds):\n",
    "    softmaxed = torch.softmax(preds, 0)\n",
    "    probas = torch.distributions.multinomial.Multinomial(1, softmaxed).sample()\n",
    "    return probas.argmax()\n",
    "\n",
    "def generate_text():\n",
    "    start_index = random.randint(0, len(text) - MAX_LEN - 1)\n",
    "\n",
    "    generated = ''\n",
    "    sentence = text[start_index: start_index + MAX_LEN]\n",
    "    generated += sentence\n",
    "\n",
    "    for i in range(MAX_LEN):\n",
    "        x_pred = torch.zeros((1, MAX_LEN), dtype=int)\n",
    "        for t, char in enumerate(generated[-MAX_LEN:]):\n",
    "            x_pred[0, t] = CHAR_TO_INDEX[char]\n",
    "\n",
    "        preds = model(x_pred) #.cuda())[0].cpu()\n",
    "        next_char = INDEX_TO_CHAR[sample(preds)]\n",
    "        generated = generated + next_char\n",
    "\n",
    "    print(generated[:MAX_LEN] + '|' + generated[MAX_LEN:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f3d79809",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6EV09Ast97aQ",
    "outputId": "385ca42f-74ee-41a8-8aeb-3d12d30d195b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "happiness thereupon all the evils living|vstp upfqnqnpmvtfmprjycvwmrjsoemgnsfyxqv\n"
     ]
    }
   ],
   "source": [
    "generate_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a71295e4",
   "metadata": {
    "id": "hylQYY8H_Lw2"
   },
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "20a0b563",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qshorynU9-Cx",
    "outputId": "e2b45154-9cf2-4799-d66a-2eb9fc5a92fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0. Time: 90.816, Train loss: 2.180\n",
      "lately been added a german the author of| mow uve hergus ipsuls thas trogh amourb\n",
      "Epoch 1. Time: 90.212, Train loss: 1.797\n",
      "mself that he persistently avoids it and| passe encomity of diskind and evilongo \n",
      "Epoch 2. Time: 88.347, Train loss: 1.657\n",
      "intained in europe is perhaps the best e|nvuth folitn to luokes ad outto he badio\n",
      "Epoch 3. Time: 87.753, Train loss: 1.568\n",
      " nature badly endowed and prepared so th|e act an a than platurally our everther \n",
      "Epoch 4. Time: 85.444, Train loss: 1.508\n",
      "of existence justifies itself no termina|ly itself one in result and attentaty it\n",
      "Epoch 5. Time: 84.961, Train loss: 1.461\n",
      "r when the occasion arises the fable of |the intruex a condigniziss and perpation\n",
      "Epoch 6. Time: 86.122, Train loss: 1.425\n",
      " about once before a general depression |from and sebbwles mechantoly to parpysin\n",
      "Epoch 7. Time: 86.091, Train loss: 1.395\n",
      "e is an innocence in lying which is the |very see a being somethow well kinds or \n",
      "Epoch 8. Time: 86.823, Train loss: 1.368\n",
      "piritual will to interpret everything th|e inclitic not a speciently lear doderat\n",
      "Epoch 9. Time: 85.714, Train loss: 1.348\n",
      "ey know but as the inferior natures are |sare to the propems and his happoen phil\n",
      "Epoch 10. Time: 86.272, Train loss: 1.327\n",
      "ncivilities something that goes its way |by antibrical objectary which actal as i\n",
      "Epoch 11. Time: 85.220, Train loss: 1.311\n",
      "equences above animal the beast in us mu|ch with in stinded to seems to limant in\n",
      "Epoch 12. Time: 84.384, Train loss: 1.296\n",
      "en who feel a sense of injury are in the| confinical skent nevertiant is without \n",
      "Epoch 13. Time: 84.894, Train loss: 1.282\n",
      "it is too bad always the old story when |cause of themselves but religious been i\n",
      "Epoch 14. Time: 85.209, Train loss: 1.271\n",
      "ounce or to share our responsibilities t|here is a centuress and some he is act a\n",
      "Epoch 15. Time: 84.829, Train loss: 1.258\n",
      "nvincible stable and the like to such an| agrioured to way us a prirence and more\n",
      "Epoch 16. Time: 85.405, Train loss: 1.249\n",
      "ble means even of spiritual education an|d fast not more compresed agnereanity by\n",
      "Epoch 17. Time: 85.003, Train loss: 1.238\n",
      " the authorities in matters of sanctity |anothers even he lover of the garm to th\n",
      "Epoch 18. Time: 84.657, Train loss: 1.231\n",
      "erning certain religious things for exam|es than it callow of will with at it it \n",
      "Epoch 19. Time: 86.737, Train loss: 1.220\n",
      "e nature of a finer turning or better ex|ercion away flarined to yould spocricati\n",
      "Epoch 20. Time: 85.011, Train loss: 1.213\n",
      "ness to apply to nature the same strict |friend nothing all greatable one areigan\n",
      "Epoch 21. Time: 85.025, Train loss: 1.206\n",
      " to ascertain if the wailing and sighing| it gloone itshinct of single for it wen\n",
      "Epoch 22. Time: 85.408, Train loss: 1.197\n",
      "being who is thought of as a cause that |one betray and their cause but our fears\n",
      "Epoch 23. Time: 84.560, Train loss: 1.191\n",
      "e herding animal things have reached suc|h as a new buded untiged and almost art \n",
      "Epoch 24. Time: 84.829, Train loss: 1.184\n",
      "ence that decided for them was low indee|d equally inspired the air far for this \n",
      "Epoch 25. Time: 84.791, Train loss: 1.177\n",
      "tion of material the comprehensive surve| their truether muy the moral that even \n",
      "Epoch 26. Time: 84.865, Train loss: 1.172\n",
      "e all that is strenuous in his circumsta|tion what the satecal born of partions o\n",
      "Epoch 27. Time: 84.911, Train loss: 1.165\n",
      "mpossible to think of mortal power havin|g tmany indivedual aftirty of moral righ\n",
      "Epoch 28. Time: 84.748, Train loss: 1.159\n",
      "s which ripen too late it flows broad an|d unextuition of him it will that consta\n",
      "Epoch 29. Time: 85.301, Train loss: 1.155\n",
      " the skeptics as lovers of repose and al|ways have nateron in a supersting things\n",
      "Epoch 30. Time: 85.280, Train loss: 1.148\n",
      "imself any other value than that which h|e sumbander as to find in the sampathy d\n",
      "Epoch 31. Time: 86.676, Train loss: 1.143\n",
      "nowing and reasoning intelligence were t|he something question significancy form \n",
      "Epoch 32. Time: 84.912, Train loss: 1.138\n",
      "querade and perhaps without being themse|la has once the reamed the exclusive asi\n",
      "Epoch 33. Time: 84.551, Train loss: 1.132\n",
      "sands of years have discovered the most |natione community known the present at m\n",
      "Epoch 34. Time: 85.145, Train loss: 1.126\n",
      " occasions pain therefore it is false th|oughtly in an able of his munifests upon\n",
      "Epoch 35. Time: 84.886, Train loss: 1.122\n",
      "t like a sensitive dull home body remain|m anfusure or the interpretations of the\n",
      "Epoch 36. Time: 84.477, Train loss: 1.118\n",
      "re comprehensive life is lived beyond th|e claticicism his head or else is a tent\n",
      "Epoch 37. Time: 86.682, Train loss: 1.112\n",
      "see that the richness of inner rational |of all great is gratiaising imment of th\n",
      "Epoch 38. Time: 86.104, Train loss: 1.109\n",
      "lla is called enthusiasm including what |always with its reliam of such an a hume\n",
      "Epoch 39. Time: 84.925, Train loss: 1.104\n",
      "doubt that as men argue in their dreams |creadicy dispension less of such spirit \n",
      "Epoch 40. Time: 85.089, Train loss: 1.099\n",
      "t indifferently in germany itself and li|ving intenter sacult of victory just but\n",
      "Epoch 41. Time: 84.471, Train loss: 1.096\n",
      " of spirit it is at the same time subjec|t understand than them to things that er\n",
      "Epoch 42. Time: 85.414, Train loss: 1.093\n",
      "uding self regulation assimilation nutri|tnal and finest uschefour simply a way w\n",
      "Epoch 43. Time: 84.882, Train loss: 1.088\n",
      "ralize what will be the common element i|t restrinction that everything consequen\n",
      "Epoch 44. Time: 84.745, Train loss: 1.084\n",
      "the powerful would have shown himself po|or things i recent of all might whatever\n",
      "Epoch 45. Time: 85.334, Train loss: 1.079\n",
      "rs may attain far reaching results for m|overnacle the ornium after obscient mad \n",
      "Epoch 46. Time: 84.654, Train loss: 1.076\n",
      "in where even we can still be original p|sychologists and heart and clossibly app\n",
      "Epoch 47. Time: 84.576, Train loss: 1.072\n",
      "iminariness probably something of the sa|cre and covetely been its religion hamer\n",
      "Epoch 48. Time: 84.727, Train loss: 1.068\n",
      "alled them dionysiokolakes in its origin|g in easily can be master every way but \n",
      "Epoch 49. Time: 84.755, Train loss: 1.065\n",
      "pectacle of the tartuffery of old kant e|xistence artorism also more gives tifree\n",
      "Epoch 50. Time: 84.892, Train loss: 1.061\n",
      " for states approximating the animal whe|n the deplick of the strength has danger\n",
      "Epoch 51. Time: 84.073, Train loss: 1.057\n",
      "eparation for a theory of types of moral|ity about the instinct of justify the ty\n",
      "Epoch 52. Time: 84.985, Train loss: 1.053\n",
      "s free from responsibility as if plants |intellect of their tipic aftern life inc\n",
      "Epoch 53. Time: 84.091, Train loss: 1.050\n",
      "l at most err through lack of knowledge |to have they have thus awaken at a wholi\n",
      "Epoch 54. Time: 84.390, Train loss: 1.047\n",
      "even health to oneself for a long time i|s find has just against be contror and m\n",
      "Epoch 55. Time: 84.532, Train loss: 1.044\n",
      "o what he can and furnish what he can bu|t exercise earth of its liked whatever a\n",
      "Epoch 56. Time: 84.432, Train loss: 1.040\n",
      "untary and unconscious auto biography an|d this worshniness of delightful indicau\n",
      "Epoch 57. Time: 84.898, Train loss: 1.036\n",
      "s whether one can knowingly remain in th|e rich selects limsing may certainly the\n",
      "Epoch 58. Time: 85.018, Train loss: 1.033\n",
      "e the will nothing is so adapted to the |lather of its use spirit this same all f\n",
      "Epoch 59. Time: 84.979, Train loss: 1.029\n",
      "elves already fully occupied these good |to be movement laws in dangerously under\n",
      "Epoch 60. Time: 84.717, Train loss: 1.027\n",
      "f language in the development of civiliz|ations his view but that the morality of\n",
      "Epoch 61. Time: 85.203, Train loss: 1.024\n",
      "hat requires sagacity and acute senses r|emainness of all earthly matters of men \n",
      "Epoch 62. Time: 84.741, Train loss: 1.022\n",
      "for securing immunity from the unavoidab|ly it detwicill shonly and had been to m\n",
      "Epoch 63. Time: 84.775, Train loss: 1.019\n",
      "perhaps there is even an order of rank w|hom relighted in the virtues and with we\n",
      "Epoch 64. Time: 85.921, Train loss: 1.016\n",
      "truths and the scientific spirit begin t|hat the latest bainning and in the atter\n",
      "Epoch 65. Time: 84.842, Train loss: 1.013\n",
      "nd amid them the memory itself seems to |be forged wild so the uttrements are cir\n",
      "Epoch 66. Time: 84.947, Train loss: 1.010\n",
      " the actual needs of the ame moderne the| most pain process of the presence of af\n",
      "Epoch 67. Time: 84.778, Train loss: 1.007\n",
      "e and false here and there one finds a p|ligrats and course as false floessibly f\n",
      "Epoch 68. Time: 85.188, Train loss: 1.005\n",
      " provocation to constant misunderstandin|g it pluded only has as an end and noble\n",
      "Epoch 69. Time: 85.274, Train loss: 1.001\n",
      " the degree of danger from themselves in|voltited if enlicistule that the resting\n",
      "Epoch 70. Time: 85.599, Train loss: 0.999\n",
      "ards every system of utilitarianism one |enored somether the precisely and peache\n",
      "Epoch 71. Time: 85.118, Train loss: 0.996\n",
      "nd his despection or he gets aloft too l|ong the religious remirisse of christian\n",
      "Epoch 72. Time: 87.903, Train loss: 0.994\n",
      "ng falsely viewed and his personality be|cause its presence of all got him to men\n",
      "Epoch 73. Time: 86.240, Train loss: 0.991\n",
      " the spirit resembles a stomach more tha|t this a popusion of one s insitting imp\n",
      "Epoch 74. Time: 84.892, Train loss: 0.988\n",
      "a of the fact that philosophizing concer|ness of the law assumed to a tempory eve\n",
      "Epoch 75. Time: 84.813, Train loss: 0.986\n",
      "busy in preserving and sanctifying image| both the highest organizat do not have \n",
      "Epoch 76. Time: 85.527, Train loss: 0.984\n",
      "ation of sir theodore martin at any rate| materially acquire stries he would not \n",
      "Epoch 77. Time: 85.058, Train loss: 0.982\n",
      "or suggestion which is generally their h|addly so many subjects he we sus explect\n",
      "Epoch 78. Time: 85.093, Train loss: 0.979\n",
      "ing that will be hidden from no augur or|ialies in wish the good diew what is pro\n",
      "Epoch 79. Time: 86.828, Train loss: 0.977\n",
      "some steady process of science which now| how errors and fundamentally instinctiv\n",
      "Epoch 80. Time: 87.949, Train loss: 0.975\n",
      "t has been well learnt it takes place th|e fathe like the fradife loging breaks e\n",
      "Epoch 81. Time: 85.408, Train loss: 0.972\n",
      "e charm of success hence present day rea|dy domain be reverence and past and perh\n",
      "Epoch 82. Time: 85.860, Train loss: 0.970\n",
      "cludes by generalizing upon its experien|ce of an among he can down be intellect \n",
      "Epoch 83. Time: 85.924, Train loss: 0.968\n",
      " order to thwart this natural all too na|ture individuals as a merely such a subj\n",
      "Epoch 84. Time: 85.089, Train loss: 0.966\n",
      "ves with the uniformity of a pendulum to| inception you esatimy in the same of lo\n",
      "Epoch 85. Time: 85.784, Train loss: 0.963\n",
      "ark well very good now let us set our te|epled in addet man again judged in which\n",
      "Epoch 86. Time: 84.180, Train loss: 0.961\n",
      "omes suddenly mad breaks the plates upse|s to soot a sideated by order the kinds \n",
      "Epoch 87. Time: 85.113, Train loss: 0.960\n",
      "ut any falsification taking place either| pleasely then the self difficult excust\n",
      "Epoch 88. Time: 84.972, Train loss: 0.957\n",
      "ell constructed and happy commonwealth n|ear attest all estimabli ear who with a \n",
      "Epoch 89. Time: 85.272, Train loss: 0.955\n",
      "ne was that a work for your hands how yo|u way we order to every whole expresses \n",
      "Epoch 90. Time: 84.882, Train loss: 0.954\n",
      "toric training forces one to acknowledge| pitumonaric nature against the interrec\n",
      "Epoch 91. Time: 84.597, Train loss: 0.954\n",
      "ven in every desire for knowledge there |is perhaps the more religion is the doma\n",
      "Epoch 92. Time: 85.465, Train loss: 0.950\n",
      "ived that an excitement of some kind oft|en of litter as our european ideas or ou\n",
      "Epoch 93. Time: 85.154, Train loss: 0.948\n",
      "ness and power of his spirituality of wh|ich all gradually contempt then in a who\n",
      "Epoch 94. Time: 84.782, Train loss: 0.946\n",
      "arned anew about heredity and innateness| and corporality sometimes upon which we\n",
      "Epoch 95. Time: 84.274, Train loss: 0.944\n",
      "r does he destroy je ne meprise presque |thereof will be german put it have been \n",
      "Epoch 96. Time: 84.976, Train loss: 0.943\n",
      "f the present day chapter iv apophthegms| in the sking than i metashed as the bel\n",
      "Epoch 97. Time: 84.511, Train loss: 0.940\n",
      "her a place of leverage that he thought |philosophers dectait couragement is in l\n",
      "Epoch 98. Time: 85.076, Train loss: 0.938\n",
      "art and this very instinct in woman we w|e are thinker they applayer himself but \n",
      "Epoch 99. Time: 85.312, Train loss: 0.937\n",
      "ferent ways the stomach carries on the d|ellightful these later of a new feignnes\n"
     ]
    }
   ],
   "source": [
    "for ep in range(100):\n",
    "    start = time.time()\n",
    "    train_loss = 0.\n",
    "    train_passed = 0\n",
    "\n",
    "    model.train()\n",
    "    for X_b, y_b in data:\n",
    "        X_b, y_b = X_b, y_b #.cuda(),.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        answers = model(X_b)\n",
    "        loss = criterion(answers, y_b)\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_passed += 1\n",
    "\n",
    "    print(\"Epoch {}. Time: {:.3f}, Train loss: {:.3f}\".format(ep, time.time() - start, train_loss / train_passed))\n",
    "    model.eval()\n",
    "    generate_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80e6697",
   "metadata": {
    "id": "zzUTHB9O_H6J"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
